<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Jennifer Cao, Claire Ding </div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-nyahello-writeups/hw3/index.html">cal-cs184-student.github.io/hw-webpages-nyahello-writeups/hw3/index.html</a>
		Link to GitHub repository: (Make sure to follow the restart branch) <a href="https://github.com/cal-cs184-student/sp25-hw3-tribbie/tree/restart">github.com/cal-cs184-student/sp25-hw3-tribbie/tree/restart</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		&emsp; In this homework, we implemented functions of a 3D rendering program that can generate images of 3D objects with realistic lighting. This was done using different types of sampling techniques alongside a pathtracting algorithm for light rays. These algorithms that handle light bounces are supported by a bounding hierarchy tree which accelerates finding where rays hit the object. Lastly, we also added adaptive sampling to improve issues with noise.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		&emsp; The first part involved using normalized image coordinates to generate a ray in camera space that we later work with to detect intersections. In order to generate that ray, we calculated a field of view for the camera filter to create a new basis for the image coordinates. Then we used a change of basis to transform the image coordinates to coordinates in the scene within the field of view of the camera sensor. Lastly, to create the ray, we took the opposite direction of the camera’s z-axis and the transformed image coordinates to world space to get the direction of the ray. Using this and the position of the camera sensor, we were able to finally generate a ray.

		<br>
		<br>
		&emsp; Alongside ray generation, we also implemented pixel sampling but also ray-object intersection functions. There were two intersections that we focused on, ray-triangle intersection and ray-sphere intersection. Ray-sphere intersection involves using a test function that finds if there is an intersection and the times at which the ray intersects. We then check the time to ensure that it is within the minimum time or maximum time of the ray. These values determine when the ray begins and ends.

		<br>
		<br>
		&emsp; The other intersection method we implemented was ray-triangle intersection. We implemented the Möller-Trumbore algorithm which calculates barycentric coordinates using the triangles points and the ray’s origin/direction. We first define three vectors, s as the ray’s origin, s1 as a dot product between the ray’s direction and one of the edges of the triangle, and s2 as the dot product between s and the other edge. We then check if the dot product of the s1 and an edge is equal to 0 to determine if the ray is parallel to the triangle’s plane. Then, we calculated barycentric coordinates b1 and b2 by getting the dot product of s1 with s and s2 with the ray’s direction and dividing both by the dot product of s1 and an edge. The time that the ray intersects can be determined by getting the dot product of s2 with the other edge and dividing by the dot product of s1 and an edge again. To check if the intersection is valid, we check if the time is between the intervals where the ray exists, and that the barycentric proportions add up to 1.

		<br>

		<p>Here are some images we rendered.</p>
		<figure>
			<img src="./images/p1_spheres.png" alt="Spheres" style="width:80%"/>
			<figcaption>Rendering of <em>CBspheres_lambertian.dae</em></figcaption>
		</figure>
		<figure>
			<img src="./images/p1_gems.png" alt="Gems" style="width:80%"/>
			<figcaption>Rendering of <em>CBgems.dae</em></figcaption>
		</figure>
		<figure>
			<img src="./images/p1_coil.png" alt="Coil" style="width:80%"/>
			<figcaption>Rendering of <em>CBcoil.dae</em></figcaption>
		</figure>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		&emsp; In part two of the homework, we worked on accelerating rendering using Bounding Volume Hierarchy. We start by creating a bounding box for the node and adding all of the primitives as well as creating the current node for the tree. Then we check if the node has too many nodes to be a leaf. If it counts as a leaf we just return the node but if it has too many primitives, then we need to split it. 
		<br>
		&emsp; The split heuristic we used ended up being checking which axis of the current bounding box is the largest. Our initial idea was to use the variance of the centroids of each point but there were issues with splitting so we switched to a simpler method. For the split heuristic, we found the axis of the overall bounding box that was the widest and assigned all primitives with centroids less than the axis value assigned to the left node of the tree and the rest to the right node.
		<br>

		<figure>
			<img src="./images/p2_cow_slow.png" alt="Cow" style="width:80%"/>
			<figcaption>Rendering of <em>cow.dae</em>. Without BVH: 23s, With BVH: 0.0907s</figcaption>
		</figure>
		<figure>
			<img src="./images/p2_plank_slow.png" alt="MaxPlanck" style="width:80%"/>
			<figcaption>Rendering of <em>maxplanck.dae</em>. Without BVH: 346s, With BVH: 0.1235s</figcaption>
		</figure>
		<figure>
			<img src="./images/p2_beetle.png" alt="Beetle" style="width:80%"/>
			<figcaption>Rendering of <em>beetle.dae</em>. Without BVH: 31s, With BVH: 0.0786s</figcaption>
		</figure>

		&emsp; The three images we chose to test were the cow, maxplanck, and beetle. Without using the acceleration from Bounding Volume Hierarchy, the cow took 23 seconds to render, but with the acceleration technique, it was able to render in 0.0907 seconds. Max planck took 346 seconds to render without BVH acceleration but with acceleration, it took only 0.1235 seconds. The last image we rendered was the beetle which took 31 seconds without BVH and 0.0786 seconds with. The main reason BVH resulted in such an improvement was because it allows intersect detection to run in logarithmic time by eliminating a lot of empty space that would not need to be checked for intersections.

		<br>

		<h2>Part 3: Direct Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<br>
		<h3>Hemisphere sampling vs Importance sampling</h3>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="./images/p3_CBbunny_hemi_low.png" width="400px"/>
				  <figcaption>Bunny rendered using <strong>Hemisphere sampling</strong> with 16 samples, 8 lights and a max ray depth of 1</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="./images/p3_bunny_imp_low2.png" width="400px"/>
				  <figcaption>Bunny rendered using <strong>Importance sampling</strong> with 16 samples, 8 lights and a max ray detpth of 1</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="./images/p3_CBbunny_hemi_high.png" width="400px"/>
				  <figcaption>Bunny rendered using <strong>Hemisphere sampling</strong> with 64 samples, 32 lights and a max ray depth of 6</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="./images/p3_bunny_imp_high.png" width="400px"/>
				  <figcaption>Bunny rendered using <strong>Importance sampling</strong> with 64 samples, 32 lights and a max ray depth of 6</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		<br>

		<h3>Importance Sampling with Varying Lights</h3>
		<figure>
			<img src="./images/p3_bunny_shadow1.png" style="width:80%"/>
			<figcaption>Bunny rendered with importance sampling with 1 light</figcaption>
		</figure>
		<figure>
			<img src="./images/p3_bunny_shadow4.png" style="width:80%"/>
			<figcaption>Bunny rendered with importance sampling with 4 lights</figcaption>
		</figure>
		<figure>
			<img src="./images/p3_bunny_shadow16.png" style="width:80%"/>
			<figcaption>Bunny rendered with importance sampling with 16 lights</figcaption>
		</figure>
		<figure>
			<img src="./images/p3_bunny_shadow64.png" style="width:80%"/>
			<figcaption>Bunny rendered with importance sampling with 64 lights</figcaption>
		</figure>
		<br>

		<br>
		&emsp; Overall, hemisphere sampling had a lot more noise scattered throughout the image than any of the importance sampling renders. This is due to how hemisphere sampling assumes the existence of light rays in directions where there are no light sources which then gets added to the sample as noise. However, importance sampling avoids this by only sampling the lights in the scene which made it produce a much cleaner result because it avoids adding rays that don’t actually contribute light.
		<br>

		<h2>Part 4: Global Illumination</h2>
		The at_least_one_bounce_radiance function computes the radiance at a surface point due to both direct and indirect illumination, and is called recursively to simulate multiple light bounces. It begins by constructing a local coordinate frame from the surface normal and transforming the outgoing ray direction into local space. It samples an incoming direction using the BSDF, retrieves the BSDF value and corresponding PDF, and terminates early if the PDF is zero or if the ray has no remaining depth. It then generates a new ray in world space, offset slightly to avoid self-intersection, and decrements its depth. The function always adds the direct lighting at the current intersection using one_bounce_radiance. For indirect lighting, it uses Russian Roulette to probabilistically terminate rays and reduce computation. If the new ray intersects a surface and survives Russian Roulette, it recursively calls itself to compute indirect lighting. If the BSDF is delta-distributed (e.g., mirror), it also adds zero_bounce_radiance. The final contribution from the bounce is weighted by the BSDF value, cosine of the incident angle, PDF, and Russian Roulette probability, and added to the total radiance output.
		<h3>Some images rendered with global (direct and indirect) illumination with 1024 samples per pixel.</h3>
		<figure>
			<img src="./images/p4_bunny.png" style="width:80%"/>
			<figcaption>Bunny rendered with 1024 samples per pixel</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bench.png" style="width:80%"/>
			<figcaption>Bench rendered with 1024 samples per pixel</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_sphere.png" style="width:80%"/>
			<figcaption>Spheres rendered with 1024 samples per pixel</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_empty_bunny.png" style="width:80%"/>
			<figcaption>Bunny rendered with 1024 samples per pixel</figcaption>
		</figure>
		<br>
		<h3>Bunny with only direct illumination, then only indirect illumination, with 1024 samples per pixel.</h3>
		<figure>
			<img src="./images/p4_bunny_direct.png" style="width:80%"/>
			<figcaption>Bunny rendered with direct illumination and 1024 samples per pixel</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_indirect.png" style="width:80%"/>
			<figcaption>Bunny rendered with indirect illumination and 1024 samples per pixel</figcaption>
		</figure>
		<br>
		<h3>For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), and isAccumBounces=false. </h3>
			For the 2nd bounce (-m 2), the image begins to reveal subtle indirect lighting effects soft illumination appears in shadowed areas, especially around crevices or areas blocked from direct light. This bounce captures light that has reflected off one surface before hitting the visible geometry, contributing to soft ambient fills that improve depth perception.

			By the 3rd bounce (-m 3), these indirect effects are even more pronounced. Surfaces facing away from light sources become better lit due to multiple light inter-reflections, enhancing the realism of global illumination. The image starts to feel more cohesive and less flat, and color bleeding (where light picks up color from one surface and affects others) becomes more noticeable.
		<figure>
			<img src="./images/p4_bunny_m5_o0.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 5 and isAccumBounces = False</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m4_o0.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 4 and isAccumBounces = False</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m3_o0.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 3 and isAccumBounces = False</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m2_o0.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 2 and isAccumBounces = False</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m1_o0.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 1 and isAccumBounces = False</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m0_o0.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 0 and isAccumBounces = False</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m1_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 1 and isAccumBounces = True</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m2_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 2 and isAccumBounces = True</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m3_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 3 and isAccumBounces = True</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m4_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 4 and isAccumBounces = True</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m5_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 5 and isAccumBounces = True</figcaption>
		</figure>
		<br>
		<h3>For CBbunny.dae, output the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4, and 100(the -m flag). Use 1024 samples per pixel.</h3>
		<figure>
			<img src="./images/p4_bunny_m0_o0.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 0</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m1_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 1</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m2_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 2</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m3_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 3</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m4_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 4</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_bunny_m100_o1.png" style="width:80%"/>
			<figcaption>Bunny rendered with max_ray_depth = 100</figcaption>
		</figure>
		<h3>Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.</h3>
		<figure>
			<img src="./images/p4_sphere_s1_l4.png" style="width:80%"/>
			<figcaption>Sphere with sample rate 1</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_sphere_s2_l4.png" style="width:80%"/>
			<figcaption>Sphere with sample rate 2</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_sphere_s4_l4.png" style="width:80%"/>
			<figcaption>Sphere with sample rate 4</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_sphere_s8_l4.png" style="width:80%"/>
			<figcaption>Sphere with sample rate 8</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_sphere_s16_l4.png" style="width:80%"/>
			<figcaption>Sphere with sample rate 16</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_sphere_s64_l4.png" style="width:80%"/>
			<figcaption>Sphere with sample rate 64</figcaption>
		</figure>
		<figure>
			<img src="./images/p4_sphere_s1024_l4.png" style="width:80%"/>
			<figcaption>Sphere with sample rate 1024</figcaption>
		</figure>


		<br>
		<h2>Part 5: Adaptive Sampling</h2>
			Adaptive sampling is a technique used to efficiently allocate computational effort during rendering by varying the number of samples per pixel based on convergence. In the raytrace_pixel implementation, the renderer begins by initializing accumulators for radiance and luminance statistics. For each sample, it jitter-samples a point within the pixel, generates a camera ray, and computes the incoming radiance using global illumination. It tracks the sum and squared sum of luminance to estimate the mean and variance. After every fixed number of samples (a batch), it checks whether the pixel has converged by computing a 95% confidence interval around the mean luminance. If the confidence interval falls within a user-defined tolerance, sampling stops early; otherwise, it continues up to the maximum allowed samples. Once sampling is complete, the average radiance is computed and stored in the image buffer, and the number of samples taken is recorded in a separate buffer used to generate a sample rate visualization. This approach ensures that smooth, low-variance regions are sampled less, while complex or noisy regions receive more samples, improving both rendering speed and image quality.		<h3>Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.</h3>
		<figure>
			<img src="./images/p5_bunny.png" style="width:80%"/>
			<figcaption>Bunny with sample rate 2048</figcaption>
		</figure>
		<figure>
			<img src="./images/p5_bunny_rate.png" style="width:80%"/>
			<figcaption>Bunny sample rate image with sample rate 2048</figcaption>
		</figure>
		<figure>
			<img src="./images/p5_sphere.png" style="width:80%"/>
			<figcaption>Sphere with sample rate 2048</figcaption>
		</figure>
		<figure>
			<img src="./images/p5_sphere_rate.png" style="width:80%"/>
			<figcaption>Sphere sample rate image with sample rate 2048</figcaption>
		</figure>
		</div>
	</body>
</html>